# Camera Driver

This tutorial cover the basis of the integration of a new sensor in _ROS 2_ environment.
In our case we will integrate [Realsense D400 RGBDi](https://www.intelrealsense.com/introducing-intel-realsense-d400-product-family/) camera.


## Drivers

First, the integration of a sensor require to identify the driver (the piece of code permiting to communicate with a devices - hardware level) and the API (Application Programming interface).

Concretly, we mostly seek for the appropriate [librairies](https://en.wikipedia.org/wiki/Library_(computing)) correctly integrated to our system.

Id√©aly the community already support the desired librairies (like for [libsdl2](https://www.libsdl.org/) for instance, simple C lib "to provide low level access to audio, keyboard, mouse, joystick, and graphics hardware").
By searching for `libsdl2` with Ubuntu-Aptitude we will find several packages ready to be installed:

```sh
apt search libsdl2
```

- `libsdl2-x.x` runing librairies (installed if programs use _SDL2_)
- `libsdl2-dev` development file (to install if you plan to develop a program based on _SDL2_)
- and some extra libs.

Other wise, we have to build/compile the driver from source code.
In case of realsense, [librealsense](https://github.com/IntelRealSense/librealsense) recommand to use `vcpkg` to build and install it.

Normally, after installation, you can start `realsense-viewer` program and connect a camera: 

```sh
realsense-viewer
```

And run a small script to request the cam (more on [dev.intelrealsense.com](https://dev.intelrealsense.com/docs/python2)): 

```python
#!/usr/bin/env python3

###############################################
##              Simple Request               ##
###############################################

import pyrealsense2 as rs

# Configure depth and color streams
pipeline = rs.pipeline()
config = rs.config()

# Get device product line for setting a supporting resolution
pipeline_wrapper = rs.pipeline_wrapper(pipeline)
pipeline_profile = config.resolve(pipeline_wrapper)
device = pipeline_profile.get_device()
device_product_line = str(device.get_info(rs.camera_info.product_line))

print( f"Connect: {device_product_line}" )
for s in device.sensors:
    print( "Name:" + s.get_info(rs.camera_info.name) )
```

Try this script with severals cameras, not all the Realsense provide for IMU (accelerations) information.

## OpenCV2 - the queen of the vision librairie.

Next we can try to visualise the image flux in a windows.
For that we will use [OpenCV2](https://opencv.org/) librairy (an open source computer vision library).

The next script, adapted from the oficial documentation, connect the camera, and display both the image and distance image in an infinite loop (`while True`).

1. Based on librealsense, the script activates the expected data flux, with the wanted configuration (848x480 imagein a given format at 60 Hertz):

```python
config.enable_stream(rs.stream.color, 848, 480, rs.format.bgr8, 60)
config.enable_stream(rs.stream.depth, 848, 480, rs.format.z16, 60)
```

2. Start the aquisition process (`pipeline.start(config)`)

3. Still with librealsense, the script wait for incomming data and get them:

```
frames = pipeline.wait_for_frames()

depth_frame = frames.first(rs.stream.depth)
color_frame = frames.first(rs.stream.color)
```

4. Then, the reminder of the script consists in converting and displaying the data based on _Numpy_ and _OpenCV_


```python
#!/usr/bin/env python3
## Doc: https://dev.intelrealsense.com/docs/python2

###############################################
##      Open CV and Numpy integration        ##
###############################################

import pyrealsense2 as rs
import time, numpy as np
import sys, cv2

# Configure depth and color streams
pipeline = rs.pipeline()
config = rs.config()

# Get device product line for setting a supporting resolution
pipeline_wrapper = rs.pipeline_wrapper(pipeline)
pipeline_profile = config.resolve(pipeline_wrapper)
device = pipeline_profile.get_device()
device_product_line = str(device.get_info(rs.camera_info.product_line))

print( f"Connect: {device_product_line}" )
found_rgb = True
for s in device.sensors:
    print( "Name:" + s.get_info(rs.camera_info.name) )
    if s.get_info(rs.camera_info.name) == 'RGB Camera':
        found_rgb = True

if not (found_rgb):
    print("Depth camera equired !!!")
    exit(0)

config.enable_stream(rs.stream.color, 848, 480, rs.format.bgr8, 60)
config.enable_stream(rs.stream.depth, 848, 480, rs.format.z16, 60)

# Start streaming
pipeline.start(config)

try:
    count= 1
    refTime= time.process_time()
    freq= 60

    sys.stdout.write("-")

    while True:

        # Wait for a coherent tuple of frames: depth, color and accel
        frames = pipeline.wait_for_frames()

        depth_frame = frames.first(rs.stream.depth)
        color_frame = frames.first(rs.stream.color)

        if not (depth_frame and color_frame):
            continue
        
        # Convert images to numpy arrays
        depth_image = np.asanyarray(depth_frame.get_data())
        color_image = np.asanyarray(color_frame.get_data())

        # Apply colormap on depth image (image must be converted to 8-bit per pixel first)
        depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)

        depth_colormap_dim = depth_colormap.shape
        color_colormap_dim = color_image.shape

        sys.stdout.write( f"\r- {color_colormap_dim} - {depth_colormap_dim} - ({round(freq)} fps)" )

        # Show images
        images = np.hstack((color_image, depth_colormap)) # supose that depth_colormap_dim == color_colormap_dim (640x480) otherwize: resized_color_image = cv2.resize(color_image, dsize=(depth_colormap_dim[1], depth_colormap_dim[0]), interpolation=cv2.INTER_AREA)

        # Show images
        cv2.namedWindow('RealSense', cv2.WINDOW_AUTOSIZE)
        cv2.imshow('RealSense', images)
        cv2.waitKey(1)
        
        # Frequency:
        if count == 10 :
            newTime= time.process_time()
            freq= 10/((newTime-refTime))
            refTime= newTime
            count= 0
        count+= 1

finally:
    # Stop streaming
    print("\nEnding...")
    pipeline.stop()
```


## Publish sensor-data

Stating from the previous script, the goal is to encapsulated the connection to the camera into a _ROS2_ _Node_ in a `tuto_vision` package (only the camera image flux).

Considering previous developed _ROS2_ _Node_ :

- We want to keep the control on the infinite loop. That for we canot use ros `spin` function anymore, and we will prefer `spin_once`. To notice that `spin_once` block until an event coccure. It is possible to overpass that by specifiing a time out (`spin_once(myNode, timeout_sec=0.01)`).

- We will publish `sensor_msgs/image`. The [ros2 documentation](https://github.com/ros2/common_interfaces/tree/galactic/sensor_msgs) is verry poor, but [_ROS1_ wiki](http://wiki.ros.org/sensor_msgs?distro=noetic) remains valuable.
- `sensor_msgs` include [header](http://docs.ros.org/en/api/std_msgs/html/msg/Header.html) to state for spacio-temporal information. Mainly the reference frame (ie. `cam` for instance) and time. For time stamp, `get_clock()` permits to get a clock of a Node instance (`node= Node()` or `self` in case of ineritance) then `now()` and `to_msg()` methods respectivelly provide curent `time()` and convert it into a msg compliant format.

```python
msg.header.stamp = node.get_clock().now().to_msg()
```

Then it is possible to feed `sensor_msgs/image` attributs (starting with `msg.encoding= "bgr8"` seems a good idea.)

However, a librairy provides some tool to work both with ROS and OpenCV ([cv_bridge](http://wiki.ros.org/cv_bridge)).
The code for image to ROS message is:

```python
from cv_bridge import CvBridge

self.bridge=CvBridge()

msg_image = self.bridge.cv2_to_imgmsg(color_image,"bgr8")
msg_image.header.stamp = self.get_clock().now().to_msg()
msg_image.header.frame_id = "image"
self.image_publisher.publish(msg_image)
```

The code for depth image to ROS message is:

```python
from cv_bridge import CvBridge

self.bridge=CvBridge()

# Utilisation de colormap sur l'image depth de la Realsense (image convertie en 8-bit par pixel)
depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)

msg_depth = self.bridge.cv2_to_imgmsg(depth_colormap,"bgr8")
msg_depth.header.stamp = msg_image.header.stamp
msg_depth.header.frame_id = "depth"
self.depth_publisher.publish(msg_depth)
```

## Going futher: 

The script use a `try`/`finally` to 'cleanly' termnate the programme at a `ctrl-c` shutdwom. 
To notice that [Python Expection](https://www.w3schools.com/python/python_try_except.asp) is not realy designed for this purpose (ie. terminate infinit loop).
The idea is to get control on the infinit loop to ros by using `spin()` method.

- Include the connection to the camera into a ros timer.
- Modify the architecture of the script to use `spin()` intead of `spin_once()`
- Try to limit the `pipeline.wait_for_frames()` with a very small duration. We do not want block the program at this step.

